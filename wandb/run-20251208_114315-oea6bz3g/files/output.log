Using device: mps

===== TOKENIZER MODEL SUMMARY =====
Total parameters:     932,803
Trainable parameters: 932,803
Embedding table:      1024 x 128  = 131,072 parameters
Estimated memory:     3.73 MB  (FP32)
===================================

Starting fresh training
Training:   6%|██████▍                                                                                                     | 3/50 [00:59<15:52, 20.27s/it]
63
63
63
63
Epoch 1: loss=0.076759, recon=0.022766, vq=0.053992, lr=1.000000e-04
New best (0.0768), saved tokenizer to tokenizer_ms_vqvae.pt
63
63
63
63
63
63
63
63
63
63
63
Epoch 2: loss=0.192359, recon=0.027117, vq=0.165242, lr=1.000000e-04
63
63
63
63
63
63
63
63
63
63
Epoch 3: loss=0.157253, recon=0.019538, vq=0.137715, lr=1.000000e-04
63
63
63
63
63
63
63
63
Epoch 4: loss=0.116458, recon=0.011516, vq=0.104941, lr=1.000000e-04
63
Epoch 5: loss=0.010553, recon=0.001234, vq=0.009319, lr=1.000000e-04
New best (0.0106), saved tokenizer to tokenizer_ms_vqvae.pt
63
63
63
Epoch 6: loss=0.034935, recon=0.003695, vq=0.031240, lr=1.000000e-04
63
63
Epoch 7: loss=0.025170, recon=0.002470, vq=0.022701, lr=1.000000e-04
63
63
63
63
63
63
Epoch 8: loss=0.065478, recon=0.006730, vq=0.058748, lr=1.000000e-04
63
Epoch 9: loss=0.013050, recon=0.001090, vq=0.011959, lr=1.000000e-04
63
63
63
63
63
63
63
63
63
63
63
Epoch 10: loss=0.116181, recon=0.011475, vq=0.104706, lr=8.000000e-05
63
63
63
63
63
Epoch 11: loss=0.054760, recon=0.004066, vq=0.050694, lr=8.000000e-05
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
Epoch 12: loss=0.166537, recon=0.017109, vq=0.149428, lr=8.000000e-05
63
63
Epoch 13: loss=0.014296, recon=0.001590, vq=0.012706, lr=8.000000e-05
63
63
63
63
63
63
63
63
63
Epoch 14: loss=0.072026, recon=0.007320, vq=0.064706, lr=6.400000e-05
63
63
Epoch 15: loss=0.014541, recon=0.001858, vq=0.012683, lr=6.400000e-05
63
Epoch 16: loss=0.005618, recon=0.000656, vq=0.004961, lr=6.400000e-05
New best (0.0056), saved tokenizer to tokenizer_ms_vqvae.pt
63
63
63
Epoch 17: loss=0.021190, recon=0.002601, vq=0.018589, lr=6.400000e-05
Epoch 18: loss=0.000000, recon=0.000000, vq=0.000000, lr=6.400000e-05
New best (0.0000), saved tokenizer to tokenizer_ms_vqvae.pt
63
63
Traceback (most recent call last):
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1654, in __del__
    self._shutdown_workers()
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1618, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/multiprocessing/connection.py", line 1136, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
Training:  36%|██████████████████████████████████████▌                                                                    | 18/50 [05:29<09:45, 18.30s/it]
Traceback (most recent call last):
  File "/Users/maximgluhovskoi/Desktop/Desktop_sub/10723/project/cmu-10X23/train_tokenizer.py", line 243, in <module>
    main()
  File "/Users/maximgluhovskoi/Desktop/Desktop_sub/10723/project/cmu-10X23/train_tokenizer.py", line 193, in main
    tokens, vq_loss, z_q = model.encode_tokens(vids)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/Desktop/Desktop_sub/10723/project/cmu-10X23/tokenizer_ms_vqvae.py", line 197, in encode_tokens
    z_q_st, indices, vq_loss = self.vq(z)  # indices: (B*T,16,16)
                               ^^^^^^^^^^
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/maximgluhovskoi/Desktop/Desktop_sub/10723/project/cmu-10X23/tokenizer_ms_vqvae.py", line 58, in forward
    dead_codes = (usage < 10).nonzero(as_tuple=False).view(-1)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
